/home/chun/.virtualenvs/tf_coral/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.8.0 and strictly below 2.11.0 (nightly versions are not supported). 
 The versions of TensorFlow you are currently using is 2.7.0 and is not supported. 
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. 
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  warnings.warn(
2022-10-26 22:18:44.403365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-26 22:18:44.407923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-26 22:18:44.408424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-26 22:18:44.409197: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-10-26 22:18:44.409704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-26 22:18:44.410197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-26 22:18:44.410676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-26 22:18:44.706578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-26 22:18:44.707167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-26 22:18:44.707690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-26 22:18:44.708163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10051 MB memory:  -> device: 0, name: GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
I1026 22:18:44.711488 139957751662400 mirrored_strategy.py:376] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
INFO:tensorflow:Maybe overwriting train_steps: 2000
I1026 22:18:44.713861 139957751662400 config_util.py:552] Maybe overwriting train_steps: 2000
INFO:tensorflow:Maybe overwriting use_bfloat16: False
I1026 22:18:44.713943 139957751662400 config_util.py:552] Maybe overwriting use_bfloat16: False
I1026 22:18:44.794650 139957751662400 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet EfficientNet backbone version: efficientnet-b0
I1026 22:18:44.794737 139957751662400 ssd_efficientnet_bifpn_feature_extractor.py:151] EfficientDet BiFPN num filters: 64
I1026 22:18:44.794794 139957751662400 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num iterations: 3
I1026 22:18:44.797224 139957751662400 efficientnet_model.py:143] round_filter input=32 output=32
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1026 22:18:44.806456 139957751662400 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1026 22:18:44.807442 139957751662400 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1026 22:18:44.808852 139957751662400 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1026 22:18:44.809369 139957751662400 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1026 22:18:44.813169 139957751662400 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1026 22:18:44.861579 139957751662400 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1026 22:18:44.864880 139957751662400 efficientnet_model.py:143] round_filter input=32 output=32
I1026 22:18:44.864953 139957751662400 efficientnet_model.py:143] round_filter input=16 output=16
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1026 22:18:44.871874 139957751662400 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1026 22:18:44.872359 139957751662400 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1026 22:18:44.873165 139957751662400 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1026 22:18:44.873562 139957751662400 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1026 22:18:44.909779 139957751662400 efficientnet_model.py:143] round_filter input=16 output=16
I1026 22:18:44.909857 139957751662400 efficientnet_model.py:143] round_filter input=24 output=24
I1026 22:18:45.017808 139957751662400 efficientnet_model.py:143] round_filter input=24 output=24
I1026 22:18:45.017898 139957751662400 efficientnet_model.py:143] round_filter input=40 output=40
I1026 22:18:45.125461 139957751662400 efficientnet_model.py:143] round_filter input=40 output=40
I1026 22:18:45.125550 139957751662400 efficientnet_model.py:143] round_filter input=80 output=80
I1026 22:18:45.286860 139957751662400 efficientnet_model.py:143] round_filter input=80 output=80
I1026 22:18:45.286951 139957751662400 efficientnet_model.py:143] round_filter input=112 output=112
I1026 22:18:45.447788 139957751662400 efficientnet_model.py:143] round_filter input=112 output=112
I1026 22:18:45.447880 139957751662400 efficientnet_model.py:143] round_filter input=192 output=192
I1026 22:18:45.660345 139957751662400 efficientnet_model.py:143] round_filter input=192 output=192
I1026 22:18:45.660438 139957751662400 efficientnet_model.py:143] round_filter input=320 output=320
I1026 22:18:45.714189 139957751662400 efficientnet_model.py:143] round_filter input=1280 output=1280
I1026 22:18:45.736587 139957751662400 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')
WARNING:tensorflow:From /home/chun/workspaces/tpu/DeepPiCar/models/research/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
Instructions for updating:
rename to distribute_datasets_from_function
W1026 22:18:45.756683 139957751662400 deprecation.py:341] From /home/chun/workspaces/tpu/DeepPiCar/models/research/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
Instructions for updating:
rename to distribute_datasets_from_function
INFO:tensorflow:Reading unweighted datasets: ['/home/chun/workspaces/tpu/DeepPiCar/train_data_generation/data/drive_with_keypress/32/annotations/train.record']
I1026 22:18:45.758483 139957751662400 dataset_builder.py:162] Reading unweighted datasets: ['/home/chun/workspaces/tpu/DeepPiCar/train_data_generation/data/drive_with_keypress/32/annotations/train.record']
INFO:tensorflow:Reading record datasets for input file: ['/home/chun/workspaces/tpu/DeepPiCar/train_data_generation/data/drive_with_keypress/32/annotations/train.record']
I1026 22:18:45.758590 139957751662400 dataset_builder.py:79] Reading record datasets for input file: ['/home/chun/workspaces/tpu/DeepPiCar/train_data_generation/data/drive_with_keypress/32/annotations/train.record']
INFO:tensorflow:Number of filenames to read: 1
I1026 22:18:45.758643 139957751662400 dataset_builder.py:80] Number of filenames to read: 1
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W1026 22:18:45.758679 139957751662400 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /home/chun/workspaces/tpu/DeepPiCar/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.
W1026 22:18:45.759871 139957751662400 deprecation.py:341] From /home/chun/workspaces/tpu/DeepPiCar/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.
WARNING:tensorflow:From /home/chun/workspaces/tpu/DeepPiCar/models/research/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
W1026 22:18:45.772720 139957751662400 deprecation.py:341] From /home/chun/workspaces/tpu/DeepPiCar/models/research/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
WARNING:tensorflow:From /home/chun/.virtualenvs/tf_coral/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1096: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
W1026 22:18:48.738667 139957751662400 deprecation.py:341] From /home/chun/.virtualenvs/tf_coral/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1096: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
WARNING:tensorflow:From /home/chun/.virtualenvs/tf_coral/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:465: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W1026 22:18:50.435321 139957751662400 deprecation.py:341] From /home/chun/.virtualenvs/tf_coral/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:465: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
2022-10-26 22:19:04.022750: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8101
2022-10-26 22:19:04.443301: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
2022-10-26 22:19:04.444437: W tensorflow/stream_executor/gpu/asm_compiler.cc:230] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6
2022-10-26 22:19:04.444458: W tensorflow/stream_executor/gpu/asm_compiler.cc:233] Used ptxas at ptxas
2022-10-26 22:19:04.444512: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2022-10-26 22:19:04.943531: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2022-10-26 22:19:04.943587: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
/home/chun/.virtualenvs/tf_coral/lib/python3.8/site-packages/keras/backend.py:414: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.
  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '
WARNING:tensorflow:From /home/chun/.virtualenvs/tf_coral/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Use fn_output_signature instead
W1026 22:19:07.824094 139938775406336 deprecation.py:545] From /home/chun/.virtualenvs/tf_coral/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Use fn_output_signature instead
WARNING:tensorflow:Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?
W1026 22:19:11.445270 139938775406336 utils.py:76] Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?
WARNING:tensorflow:Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?
W1026 22:19:16.166953 139938775406336 utils.py:76] Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?
WARNING:tensorflow:Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?
W1026 22:19:20.352571 139938775406336 utils.py:76] Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?
WARNING:tensorflow:Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?
W1026 22:19:24.906013 139938775406336 utils.py:76] Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?
INFO:tensorflow:Step 100 per-step time 0.838s
I1026 22:20:31.440034 139957751662400 model_lib_v2.py:705] Step 100 per-step time 0.838s
INFO:tensorflow:{'Loss/classification_loss': 3.231649,
 'Loss/localization_loss': 0.030875785,
 'Loss/regularization_loss': 0.028343372,
 'Loss/total_loss': 3.290868,
 'learning_rate': 0.00416}
I1026 22:20:31.440265 139957751662400 model_lib_v2.py:708] {'Loss/classification_loss': 3.231649,
 'Loss/localization_loss': 0.030875785,
 'Loss/regularization_loss': 0.028343372,
 'Loss/total_loss': 3.290868,
 'learning_rate': 0.00416}
INFO:tensorflow:Step 200 per-step time 0.517s
I1026 22:21:23.100202 139957751662400 model_lib_v2.py:705] Step 200 per-step time 0.517s
INFO:tensorflow:{'Loss/classification_loss': 4.0572777,
 'Loss/localization_loss': 0.011097515,
 'Loss/regularization_loss': 0.028357795,
 'Loss/total_loss': 4.096733,
 'learning_rate': 0.0073200003}
I1026 22:21:23.100409 139957751662400 model_lib_v2.py:708] {'Loss/classification_loss': 4.0572777,
 'Loss/localization_loss': 0.011097515,
 'Loss/regularization_loss': 0.028357795,
 'Loss/total_loss': 4.096733,
 'learning_rate': 0.0073200003}
INFO:tensorflow:Step 300 per-step time 0.518s
I1026 22:22:14.850233 139957751662400 model_lib_v2.py:705] Step 300 per-step time 0.518s
INFO:tensorflow:{'Loss/classification_loss': 1.3998764,
 'Loss/localization_loss': 0.0048925863,
 'Loss/regularization_loss': 0.028371492,
 'Loss/total_loss': 1.4331404,
 'learning_rate': 0.010480001}
I1026 22:22:14.850412 139957751662400 model_lib_v2.py:708] {'Loss/classification_loss': 1.3998764,
 'Loss/localization_loss': 0.0048925863,
 'Loss/regularization_loss': 0.028371492,
 'Loss/total_loss': 1.4331404,
 'learning_rate': 0.010480001}
